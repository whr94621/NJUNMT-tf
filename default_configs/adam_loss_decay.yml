# baseline optimizer
# Adam optimizer
# automatically decaying from learning_rate to min_learning_rate
#  according to loss on devset

optimizer_params:
  optimizer.name: Adam
  optimizer.params:
    epsilon: 8.0e-7
  optimizer.learning_rate: 5.0e-4
  optimizer.clip_gradients: 1.0
  
  optimizer.lr_decay:
    decay_type: loss_decay
    patience: 30
    decay_steps: 100
    decay_rate: 0.99
    start_decay_at: 0
    stop_decay_at: 1000000000
    min_learning_rate: 5.0e-5
    staircase: false
